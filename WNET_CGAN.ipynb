{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Activation, Dense, Input\n",
    "from keras.layers import Conv2D, Flatten\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "\n",
    "from keras.optimizers import adam\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Conv2DTranspose\n",
    "from keras.layers import Input, AveragePooling2D, MaxPooling2D, Dropout, Lambda, AlphaDropout\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import rasterio\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trainable(net, val):\n",
    "    # net.trainable = val\n",
    "    for l in net.layers:\n",
    "        l.trainable = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wnet(inp_dsm, inp_pan, blocks_list, k_size, activation, n_labels=1, name=None):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        n_labels, int, number of labels = 1\n",
    "        blocks list, list, number of filters in each block\n",
    "        k_size, tuple, filter size\n",
    "        activation, string, activation function\n",
    "        \n",
    "    output:\n",
    "        keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # PAN\n",
    "    \n",
    "    k_init = 'lecun_normal'\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        concat_axis = 1\n",
    "    else:\n",
    "        concat_axis = 3\n",
    "        \n",
    "    encoder_pan = inp_pan\n",
    "    \n",
    "    list_encoders = []\n",
    "    \n",
    "    print('Building Unet for PAN Image')\n",
    "    print(blocks_list)   \n",
    "    \n",
    "    with K.name_scope('PAN_UNet'):\n",
    "        for l_idx, n_ch in enumerate(blocks_list):\n",
    "            with K.name_scope('Encoder_block_{0}'.format(l_idx)):\n",
    "                encoder_pan = Conv2D(filters=n_ch,\n",
    "                                 kernel_size=k_size,\n",
    "                                 activation=activation,\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=k_init)(encoder_pan)\n",
    "                encoder_pan = AlphaDropout(0.1*l_idx, )(encoder_pan)\n",
    "                encoder_pan = Conv2D(filters=n_ch,\n",
    "                                 kernel_size=k_size,\n",
    "                                 dilation_rate=(2, 2),\n",
    "                                 activation=activation,\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=k_init)(encoder_pan)\n",
    "                list_encoders.append(encoder_pan)\n",
    "                # add maxpooling layer except the last layer\n",
    "                if l_idx < len(blocks_list) - 1:\n",
    "                    encoder_pan = MaxPooling2D(pool_size=(2,2))(encoder_pan)\n",
    "                # if use_tfboard:\n",
    "                    # tf.summary.histogram('conv_encoder', encoder)\n",
    "        # decoders\n",
    "        decoder_pan = encoder_pan\n",
    "        dec_n_ch_list = blocks_list[::-1][1:]\n",
    "        print(dec_n_ch_list)\n",
    "        for l_idx, n_ch in enumerate(dec_n_ch_list):\n",
    "            with K.name_scope('Decoder_block_{0}'.format(l_idx)):\n",
    "                l_idx_rev = len(blocks_list) - 1 - l_idx\n",
    "                decoder_pan = concatenate([decoder_pan, list_encoders[l_idx_rev]], axis=concat_axis)\n",
    "                decoder_pan = Conv2D(filters=n_ch,\n",
    "                                 kernel_size=k_size,\n",
    "                                 activation=activation,\n",
    "                                 padding='same',\n",
    "                                 dilation_rate=(2, 2),\n",
    "                                 kernel_initializer=k_init)(decoder_pan)\n",
    "                decoder_pan = AlphaDropout(0.1*l_idx, )(decoder_pan)\n",
    "                decoder_pan = Conv2D(filters=n_ch,\n",
    "                                 kernel_size=k_size,\n",
    "                                 activation=activation,\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=k_init)(decoder_pan)\n",
    "                decoder_pan = Conv2DTranspose(filters=n_ch,\n",
    "                                          kernel_size=k_size,\n",
    "                                          strides=(2, 2), \n",
    "                                          activation=activation,\n",
    "                                          padding='same',\n",
    "                                          kernel_initializer=k_init)(decoder_pan)\n",
    "\n",
    "        # output layer should be softmax\n",
    "        outp_pan = Conv2DTranspose(filters=n_labels,\n",
    "                               kernel_size=k_size,\n",
    "                               activation='sigmoid',\n",
    "                               padding='same',\n",
    "                               kernel_initializer='glorot_normal')(decoder_pan)\n",
    "    \n",
    "    ### DSM\n",
    "    \n",
    "    encoder_dsm = inp_dsm\n",
    "    \n",
    "    list_encoders_dsm = []\n",
    "    \n",
    "    print('Building Unet for DSM')\n",
    "    print(blocks_list)   \n",
    "    \n",
    "    with K.name_scope('DSM_UNet'):\n",
    "        for l_idx, n_ch in enumerate(blocks_list):\n",
    "            with K.name_scope('Encoder_block_{0}'.format(l_idx)):\n",
    "                encoder_dsm = Conv2D(filters=n_ch,\n",
    "                                 kernel_size=k_size,\n",
    "                                 activation=activation,\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=k_init)(encoder_dsm)\n",
    "                encoder_dsm = AlphaDropout(0.1*l_idx, )(encoder_dsm)\n",
    "                encoder_dsm = Conv2D(filters=n_ch,\n",
    "                                 kernel_size=k_size,\n",
    "                                 dilation_rate=(2, 2),\n",
    "                                 activation=activation,\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=k_init)(encoder_dsm)\n",
    "                list_encoders_dsm.append(encoder_dsm)\n",
    "                # add maxpooling layer except the last layer\n",
    "                if l_idx < len(blocks_list) - 1:\n",
    "                    encoder_dsm = MaxPooling2D(pool_size=(2,2))(encoder_dsm)\n",
    "                # if use_tfboard:\n",
    "                    # tf.summary.histogram('conv_encoder', encoder)\n",
    "        # decoders\n",
    "        decoder_dsm = encoder_dsm\n",
    "        dec_n_ch_list = blocks_list[::-1][1:]\n",
    "        print(dec_n_ch_list)\n",
    "        for l_idx, n_ch in enumerate(dec_n_ch_list):\n",
    "            with K.name_scope('Decoder_block_{0}'.format(l_idx)):\n",
    "                l_idx_rev = len(blocks_list) - 1 - l_idx\n",
    "                decoder_dsm = concatenate([decoder_dsm, list_encoders[l_idx_rev]], axis=concat_axis)\n",
    "                decoder_dsm = Conv2D(filters=n_ch,\n",
    "                                 kernel_size=k_size,\n",
    "                                 activation=activation,\n",
    "                                 padding='same',\n",
    "                                 dilation_rate=(2, 2),\n",
    "                                 kernel_initializer=k_init)(decoder_dsm)\n",
    "                decoder_dsm = AlphaDropout(0.1*l_idx, )(decoder_dsm)\n",
    "                decoder_dsm = Conv2D(filters=n_ch,\n",
    "                                 kernel_size=k_size,\n",
    "                                 activation=activation,\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=k_init)(decoder_dsm)\n",
    "                decoder_dsm = Conv2DTranspose(filters=n_ch,\n",
    "                                          kernel_size=k_size,\n",
    "                                          strides=(2, 2), \n",
    "                                          activation=activation,\n",
    "                                          padding='same',\n",
    "                                          kernel_initializer=k_init)(decoder_dsm)\n",
    "        \n",
    "        # output layer should be softmax\n",
    "        outp_dsm = Conv2DTranspose(filters=n_labels,\n",
    "                               kernel_size=k_size,\n",
    "                               activation='sigmoid',\n",
    "                               padding='same',\n",
    "                               kernel_initializer='glorot_normal')(decoder_dsm)\n",
    "        \n",
    "        outp = concatenate([outp_dsm, outp_pan], axis=concat_axis)\n",
    "        outp = Conv2D(filters=1, kernel_size=(1,1), padding='same', kernel_initializer='lecun_normal')(outp)\n",
    "\n",
    "    return Model(inputs=[inp_dsm,inp_pan], outputs=[outp], name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DiscriminatorNet(inp_DSM, inp_Label, block_list, activation, k_size=(3,3), inputs_ch=64, name='DISCR'):\n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        concat_axis = 1\n",
    "    else:\n",
    "        concat_axis = 3\n",
    "\n",
    "    k_init = 'lecun_normal'\n",
    "    with K.name_scope('DiscriminatorNet'):\n",
    "        with K.name_scope('DSM_input_conv'):\n",
    "            X = Conv2D(filters=inputs_ch,\n",
    "                       kernel_size=(1,1),\n",
    "                       activation=activation,\n",
    "                       padding='same',\n",
    "                       kernel_initializer=k_init)(inp_DSM)\n",
    "        with K.name_scope('Label_input_conv'):  \n",
    "            Y = Conv2D(filters=inputs_ch,\n",
    "                       kernel_size=(1,1),\n",
    "                       activation=activation,\n",
    "                       padding='same',\n",
    "                       kernel_initializer=k_init)(inp_Label)\n",
    "            \n",
    "        encoder = concatenate([X, Y], axis=concat_axis) \n",
    "        for l_idx, n_ch in enumerate(block_list):  #something like [32,32,32,32,32]\n",
    "            with K.name_scope('Discr_block_{0}'.format(l_idx)):\n",
    "                encoder = Conv2D(filters=n_ch,\n",
    "                                 kernel_size=k_size,\n",
    "                                 activation=activation,\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=k_init)(encoder)\n",
    "                # encoder = AlphaDropout(0.1*l_idx, )(encoder)\n",
    "                # add maxpooling layer except the last layer\n",
    "                if l_idx < len(block_list) - 1:\n",
    "                    encoder = MaxPooling2D(pool_size=(2,2))(encoder)\n",
    "        encoder = Flatten()(encoder)\n",
    "        outp = Dense(1, activation='sigmoid')(encoder)\n",
    "    \n",
    "    return Model(inputs=[inp_DSM, inp_Label], outputs=outp, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture (Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wnet_cgan:\n",
    "    def __init__(self,\n",
    "                 height, \n",
    "                 width,\n",
    "                 n_labels=1):\n",
    "        \n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            input_shape = (1, height, width) #define laebl_shape separately in case of multiple labels of roof\n",
    "            concat_axis = 1\n",
    "        else:\n",
    "            input_shape = (height, width, 1)\n",
    "            concat_axis = 3\n",
    "            \n",
    "        self.pan_shape = self.dsm_shape = self.label_shape = input_shape\n",
    "        self.init_epoch = 0\n",
    "        self.n_labels = n_labels\n",
    "        \n",
    "    def build_wnet_cgan(self,\n",
    "                        wnet_block_list,\n",
    "                        wnet_k_size, \n",
    "                        wnet_activation='selu',\n",
    "                        wnet_lr=1e-4,\n",
    "                        discr_inp_channels = 64,\n",
    "                        discr_block_list=[32,32,32,32,32],\n",
    "                        discr_k_size=(3,3), \n",
    "                        discr_activation='relu',\n",
    "                        discr_lr=1e-4,\n",
    "                        lambda_=1e-1):\n",
    "        inp_dsm = Input(self.dsm_shape, name='dsm_input')\n",
    "        inp_pan = Input(self.pan_shape, name='pan_input')\n",
    "        inp_label = Input(self.label_shape, name='label_input')\n",
    "\n",
    "        wnet_opt = adam(lr=wnet_lr)\n",
    "        discr_opt = adam(lr=discr_lr)\n",
    "\n",
    "        # build the Discriminator\n",
    "        print('Build discr')\n",
    "        self.discriminator = DiscriminatorNet(inp_dsm,\n",
    "                                              inp_label,\n",
    "                                              discr_block_list,\n",
    "                                              discr_activation,\n",
    "                                              discr_k_size,\n",
    "                                              discr_inp_channels,\n",
    "                                              'Discriminator')\n",
    "        print('Done')\n",
    "        # make Discriminator untrainable and copy it to 'frozen Discriminator' (like pyTorch's detach()?!)\n",
    "        make_trainable(self.discriminator, False)\n",
    "\n",
    "        frozen_discriminator = Model(inputs=self.discriminator.inputs,\n",
    "                                     outputs=self.discriminator.outputs,\n",
    "                                     name='frozen_discriminator')\n",
    "        frozen_discriminator.compile(discr_opt,\n",
    "                                     loss = 'binary_crossentropy',\n",
    "                                     metrics=['accuracy'])\n",
    "        #print('Frozen and compiled')\n",
    "        # build the wnet\n",
    "        #print('Build Wnet')\n",
    "        self.wnet = Wnet(inp_dsm, \n",
    "                         inp_pan, \n",
    "                         wnet_block_list, \n",
    "                         wnet_k_size, \n",
    "                         wnet_activation, \n",
    "                         self.n_labels, \n",
    "                         name='Wnet')\n",
    "\n",
    "        #compile the wnet\n",
    "        self.wnet.compile(wnet_opt,\n",
    "                          loss = 'binary_crossentropy',\n",
    "                          metrics=['accuracy'])  # CHANGE TO mIoU !!!!!!!!\n",
    "\n",
    "        #print('Compiled Wnet') \n",
    "        # get the wnet prediction\n",
    "        pred = self.wnet([inp_dsm, inp_pan])\n",
    "        #print('got pred from Wnet')\n",
    "        # input the prediction into the frozen discriminator and get the probability fake/real\n",
    "        prob = frozen_discriminator([inp_dsm, pred])\n",
    "        #print('got prob from frozen Discr')\n",
    "        # stack wnet and discriminator to form the Wnet-CGAN\n",
    "        #print('stacking the two')\n",
    "        self.wnet_cgan = Model(inputs=[inp_dsm, inp_pan, inp_label],\n",
    "                               outputs=[pred, prob],\n",
    "                               name='WNet-CGAN')\n",
    "        #print('stacked')\n",
    "        # compile it\n",
    "        #print('compiling the stcaked')\n",
    "        self.wnet_cgan.compile(wnet_opt,\n",
    "                               loss=['binary_crossentropy', 'binary_crossentropy'],\n",
    "                               loss_weights=[1., lambda_],\n",
    "                               metrics=['accuracy'])\n",
    "        #print('compiled')\n",
    "        #print(wnet_cgan.summary())\n",
    "\n",
    "        # compile the discriminator\n",
    "        make_trainable(self.discriminator, True)\n",
    "        self.discriminator.compile(discr_opt,\n",
    "                                   loss='binary_crossentropy',\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "        #print(self.discriminator.summary())\n",
    "            \n",
    "    def fit_wnet_cgan(self,\n",
    "                      train_generator,\n",
    "                      valid_generator,\n",
    "                      adv_epochs=10,\n",
    "                      adv_steps_epoch=100,\n",
    "                      gen_epochs=20,\n",
    "                      gen_steps_epoch=100,\n",
    "                      validation_steps=4,\n",
    "                      n_rounds=10):\n",
    "\n",
    "        discr_callbacks = self.build_callbacks(monitor='val_acc', phase='discr')\n",
    "        gen_callbacks = self.build_callbacks(monitor='val_acc', phase='gen')\n",
    "              \n",
    "        for i in range(n_rounds):\n",
    "            #train discriminator first\n",
    "            #self.discriminator.fit(x=discr_X, \n",
    "                                   #y=discr_Y,\n",
    "                                   #epochs=(i+1)*adv_epochs,\n",
    "                                   #callbacks=discr_callbacks,\n",
    "                                   #validation_split=0.2,\n",
    "                                   #validation_steps=validation_steps,\n",
    "                                   #shuffle=True,\n",
    "                                   #steps_per_epoch=adv_steps_epoch,\n",
    "                                   #initial_epoch=i*adv_epochs,\n",
    "                                   #verbose=0)\n",
    "            train_generator.phase='gen'\n",
    "            valid_generator.phase='gen'\n",
    "            self.wnet_cgan.fit_generator(generator=train_generator,\n",
    "                               validation_data=valid_generator,\n",
    "                               epochs=(i+1)*gen_epochs,\n",
    "                               callbacks=gen_callbacks,\n",
    "                               validation_steps=validation_steps,\n",
    "                               shuffle=True,\n",
    "                               steps_per_epoch=gen_steps_epoch,\n",
    "                               initial_epoch=i*gen_epochs,\n",
    "                               verbose=1)\n",
    "            \n",
    "            # Sub training-dataset for disciminator\n",
    "            #pred = self.wnet.predict([X[0],X[1]])\n",
    "\n",
    "            #discr_X_1, discr_X_2 = np.concatenate((X[0],X[0]), axis=0), np.concatenate((X[2],pred), axis=0)\n",
    "            #discr_Y = np.concatenate((Y[1],np.ones(shape=(len(pred),1))),axis=0)\n",
    "            \n",
    "            #discr_X_1, discr_X2, discr_Y = shuffle(discr_X_1, discr_X_2, discr_Y, random_state=42)\n",
    "            #discr_X = [discr_X_1, discr_X_2]\n",
    "            \n",
    "            self.wnet._make_predict_function()\n",
    "            train_generator.pred_fn = self.wnet.predict\n",
    "            valid_generator.pred_fn = self.wnet.predict\n",
    "            train_generator.phase='discr'\n",
    "            valid_generator.phase='discr'\n",
    "            \n",
    "            # train discriminator last\n",
    "            self.discriminator.fit_generator(generator=train_generator, \n",
    "                                   validation_data=valid_generator,\n",
    "                                   epochs=(i+1)*adv_epochs,\n",
    "                                   callbacks=discr_callbacks,\n",
    "                                   validation_steps=validation_steps,\n",
    "                                   shuffle=True,\n",
    "                                   steps_per_epoch=adv_steps_epoch,\n",
    "                                   initial_epoch=i*adv_epochs,\n",
    "                                   verbose=0)\n",
    "    \n",
    "            \n",
    "    def build_callbacks(self, use_tfboard=True, monitor=None, phase=None, save=False):\n",
    "        \n",
    "        if phase == 'gen':\n",
    "            path = './results/gen'\n",
    "        elif phase == 'discr':\n",
    "            path = './results/discr'\n",
    "\n",
    "        # Model Checkpoints\n",
    "        if monitor is None:\n",
    "            callbackList = []\n",
    "        else:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            filepath=path+'/weights-{epoch:02d}.hdf5'\n",
    "            checkpoint = ModelCheckpoint(filepath,\n",
    "                                         monitor=monitor,\n",
    "                                         verbose=1,\n",
    "                                         save_best_only=save,\n",
    "                                         save_weights_only=True,\n",
    "                                         mode='max')\n",
    "\n",
    "            # Bring all the callbacks together into a python list\n",
    "            callbackList = [checkpoint]\n",
    "                    \n",
    "        # Tensorboard\n",
    "        if use_tfboard:\n",
    "            if phase is None:\n",
    "                tfpath = './logs'\n",
    "            else:\n",
    "                tfpath = './logs/{0}'.format(phase)\n",
    "            tensorboard = TrainValTensorBoard(log_dir=tfpath)\n",
    "            callbackList.append(tensorboard)\n",
    "        return callbackList\n",
    "    \n",
    "    \n",
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs', phase=None, hist_freq=0, **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, histogram_freq=hist_freq, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, DSM_IDs,\n",
    "                 PAN_IDs,\n",
    "                 LABEL_IDs,\n",
    "                 batch_size=32,\n",
    "                 shuffle=True,\n",
    "                 pred_fn=None):\n",
    "        'Initialization'\n",
    "        self.DSM_IDs = DSM_IDs\n",
    "        self.PAN_IDs = PAN_IDs\n",
    "        self.LABEL_IDs = LABEL_IDs\n",
    "        self.phase = 'gen'\n",
    "        self.pred_fn = pred_fn\n",
    "        if len(self.PAN_IDs) != len(self.DSM_IDs) or len(self.DSM_IDs) != len(self.LABEL_IDs):\n",
    "            raise ValueError('DSM, PAN or LABEL do not match')\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.DSM_IDs) / self.batch_size))\n",
    "\n",
    "    def getitem(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        DSM_IDs_temp = [self.DSM_IDs[k] for k in indexes]\n",
    "        PAN_IDs_temp = [self.PAN_IDs[k] for k in indexes]\n",
    "        LABEL_IDs_temp = [self.LABEL_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        DSM, PAN, label = self.__data_generation(DSM_IDs_temp, PAN_IDs_temp, LABEL_IDs_temp)\n",
    "        \n",
    "        if self.phase == 'gen':\n",
    "            y1 = np.ones([label.shape[0], 1])\n",
    "            return [DSM, PAN, label], [label, y1]\n",
    "\n",
    "        elif self.phase == 'discr':\n",
    "        \n",
    "            pred = self.pred_fn([DSM,PAN])\n",
    "            \n",
    "            discr_X_1 = np.concatenate((DSM,DSM), axis=0)\n",
    "            discr_X_2 = np.concatenate((label,pred), axis=0)\n",
    "            \n",
    "            y1 = np.ones(shape=(len(label),1))\n",
    "            y0 = np.zeros(shape=(len(pred),1))\n",
    "            \n",
    "            prob = np.concatenate([y1,y0],axis=0)\n",
    "            \n",
    "            #shuffle\n",
    "            discr_X_1, discr_X2, prob = shuffle(discr_X_1, discr_X_2, prob, random_state=42)\n",
    "            \n",
    "                    \n",
    "            discr_X = [discr_X_1, discr_X_2]\n",
    "            return discr_X, prob\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        DSM_IDs_temp = [self.DSM_IDs[k] for k in indexes]\n",
    "        PAN_IDs_temp = [self.PAN_IDs[k] for k in indexes]\n",
    "        LABEL_IDs_temp = [self.LABEL_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        DSM, PAN, label = self.__data_generation(DSM_IDs_temp, PAN_IDs_temp, LABEL_IDs_temp)\n",
    "        \n",
    "        if self.phase == 'gen':\n",
    "            y1 = np.ones([label.shape[0], 1])\n",
    "            return [DSM, PAN, label], [label, y1]\n",
    "\n",
    "        elif self.phase == 'discr':\n",
    "        \n",
    "            pred = self.pred_fn([DSM,PAN])\n",
    "            \n",
    "            discr_X_1 = np.concatenate((DSM,DSM), axis=0)\n",
    "            discr_X_2 = np.concatenate((label,pred), axis=0)\n",
    "            \n",
    "            y1 = np.ones(shape=(len(label),1))\n",
    "            y0 = np.zeros(shape=(len(pred),1))\n",
    "            \n",
    "            prob = np.concatenate([y1,y0],axis=0)\n",
    "            \n",
    "            #shuffle\n",
    "            discr_X_1, discr_X2, prob = shuffle(discr_X_1, discr_X_2, prob, random_state=42)\n",
    "            \n",
    "                    \n",
    "            discr_X = [discr_X_1, discr_X_2]\n",
    "            return discr_X, prob\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.DSM_IDs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, DSM_IDs_temp, PAN_IDs_temp, LABEL_IDs_temp):\n",
    "        'Generates data containing batch_size samples' \n",
    "        # X_out : (n_samples, *dim, n_channels)\n",
    "        # Y_out : (n_samples, *dim, n_classes)\n",
    "        # Initialization\n",
    "        DSM_out = []\n",
    "        PAN_out = []\n",
    "        LABEL_out = []\n",
    "        for i in range(len(DSM_IDs_temp)):\n",
    "                DSM_out.append(np.moveaxis(rasterio.open(DSM_IDs_temp[i]).read(),0,2))\n",
    "                PAN_out.append(np.moveaxis(rasterio.open(PAN_IDs_temp[i]).read(),0,2))\n",
    "                LABEL_out.append(np.moveaxis(rasterio.open(LABEL_IDs_temp[i]).read(),0,2))\n",
    "       \n",
    "        return np.asarray(DSM_out), np.asarray(PAN_out), np.asarray(LABEL_out)\n",
    "    \n",
    "class Data:\n",
    "    \n",
    "    def __init__(self, path, random=False):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            path: path to the folder with subfolders: DSM, PAN, LABEL\n",
    "            max_num: int, num of samples\n",
    "            random: bool, to load samples randomly or from 0 to num_max\n",
    "        \"\"\"\n",
    "        self.DSM = sorted(glob.glob(path+\"/DSM/*.tif\"))\n",
    "        self.PAN = sorted(glob.glob(path+\"/PAN/*.tif\"))\n",
    "        self.LABEL = sorted(glob.glob(path+\"/LABEL/*.tif\"))\n",
    "        if len(self.DSM) != len(self.PAN) or len(self.LABEL) != len(self.PAN):\n",
    "            raise ValueError('DSM, PAN or LABEL do not match')\n",
    "      \n",
    "    def get_data(self, start=0, num=10, as_arr=True, random=False):\n",
    "        \"\"\"\n",
    "        function: load max_num of XY into lists\n",
    "        output: list of numpy arrays, X (images) and Y (labels)\n",
    "        \"\"\"\n",
    "        DSM_out = []\n",
    "        PAN_out = []\n",
    "        LABEL_out = []\n",
    "      \n",
    "        if random:\n",
    "            idx = np.random.choice(list(range(len(self.X))), num, replace=False)\n",
    "            print('randomly loading {0} tiles from {1} tiles'.format(num, len(self.DSM))) \n",
    "        else:\n",
    "            idx = list(range(start, start+num))\n",
    "            print('loading {0} - {1} image tiles'.format(start, start+num-1))\n",
    "\n",
    "        for i in idx:\n",
    "            DSM_out.append(np.moveaxis(rasterio.open(self.DSM[i]).read(),0,2))\n",
    "            PAN_out.append(np.moveaxis(rasterio.open(self.PAN[i]).read(),0,2))\n",
    "            LABEL_out.append(np.moveaxis(rasterio.open(self.LABEL[i]).read(),0,2))\n",
    "        \n",
    "        DSM_remove = [self.DSM[i] for i in idx]\n",
    "        PAN_remove = [self.PAN[i] for i in idx]\n",
    "        LABEL_remove = [self.LABEL[i] for i in idx]\n",
    "        \n",
    "        for i in range(len(DSM_remove)):\n",
    "            self.DSM.remove(DSM_remove[i])\n",
    "            self.PAN.remove(PAN_remove[i])\n",
    "            self.LABEL.remove(LABEL_remove[i])\n",
    "        \n",
    "        if as_arr:\n",
    "            return np.asarray(DSM_out), np.asarray(PAN_out), np.asarray(LABEL_out)\n",
    "        else:\n",
    "            return DSM_out, PAN_out, LABEL_out\n",
    "           \n",
    "    def split_trn_vld_tst(self, vld_rate=0.2, tst_rate=0.05, random=True, seed=10):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        num = len(self.DSM)\n",
    "        vld_num = int(num*vld_rate)\n",
    "        tst_num = int(num*tst_rate)\n",
    "        \n",
    "        print('split into {0} train, {1} validation, {2} test samples'.format(num-vld_num-tst_num, vld_num, tst_num))\n",
    "        idx = np.arange(num)\n",
    "        if random:\n",
    "            np.random.shuffle(idx)\n",
    "        DSM_tst, PAN_tst, LABEL_tst = [self.DSM[k] for k in idx[:tst_num]], [self.PAN[k] for k in idx[:tst_num]], [self.LABEL[k] for k in idx[:tst_num]]\n",
    "        DSM_vld, PAN_vld, LABEL_vld = [self.DSM[k] for k in idx[tst_num:tst_num+vld_num]], [self.PAN[k] for k in idx[tst_num:tst_num+vld_num]], [self.LABEL[k] for k in idx[tst_num:tst_num+vld_num]]\n",
    "        DSM_trn, PAN_trn, LABEL_trn = [self.DSM[k] for k in idx[tst_num+vld_num:]], [self.PAN[k] for k in idx[tst_num+vld_num:]], [self.LABEL[k] for k in idx[tst_num+vld_num:]]\n",
    "        \n",
    "        \n",
    "        return DSM_trn, PAN_trn, LABEL_trn, DSM_vld, PAN_vld, LABEL_vld, DSM_tst, PAN_tst, LABEL_tst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = Data(r'D:\\source\\TRAIN_DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm_train, pan_train, label_train, dsm_vld, pan_vld, label_vld, dsm_tst, pan_tst, label_tst = dd.split_trn_vld_tst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(dsm_train, pan_train, label_train, pred_fn=None, batch_size=8, \n",
    "                          shuffle=True)\n",
    "valid_gen = DataGenerator(dsm_vld, pan_vld, label_vld, pred_fn=None, batch_size=8, \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = Wnet_cgan(256, 256, n_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel.build_wnet_cgan([32,32,32,32],\n",
    "                        (3,3), \n",
    "                        wnet_activation='selu',\n",
    "                        wnet_lr=1e-4,\n",
    "                        discr_inp_channels = 16,\n",
    "                        discr_block_list=[32,32,32,32],\n",
    "                        discr_k_size=(3,3), \n",
    "                        discr_activation='relu',\n",
    "                        discr_lr=1e-4,\n",
    "                        lambda_=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel.fit_wnet_cgan(train_gen, valid_gen, adv_epochs=25, gen_epochs=75,\n",
    "                     adv_steps_epoch=50, gen_steps_epoch=100, n_rounds=20)\n",
    "#myModel.fit_wnet_cgan(train_gen, valid_gen, adv_epochs=2, gen_epochs=2,\n",
    "#                     adv_steps_epoch=2, gen_steps_epoch=2, n_rounds=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
